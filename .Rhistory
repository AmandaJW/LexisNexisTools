dates.d <-unique(dates)
dates.d <-dates.d[order(dates.d)]
duplicates.df <- lapply(dates.d ,FUN=function(dates.d){
# convert to quanteda dfm
text.dfm <- dfm(texts[grep(dates.d, dates)], verbose = F, tolower = TRUE)
if(is.null(IDs)){IDs <- 1:length(texts)}
text.dfm@Dimnames$docs <- IDs[grep(dates.d, dates)]
#docnames(text.dfm)
sample_sim <- as.matrix(textstat_simil(text.dfm, selection=NULL , method = "correlation",
margin="documents"))
# which values are bigger than test_sim but smaller 1 (direct duplicates are already gone)
dup <- as.data.frame(which(sample_sim > threshold & sample_sim < 1, arr.ind=TRUE))
# every pair of duplicates is in here twice. This doesn't make sense of course so let's get rid of them
if (nrow(dup)>0){
dup[,3] <- dup[,1]+dup[,2]
dup <- dup[(which(duplicated(dup[,3])==FALSE)),]
dup <- dup[,-3]
# now the row/colnumbers are replaced by article IDs (which were stored as docnames)
dup[,1] <- sapply(1:nrow(dup), function(i) dup[i,1] <- rownames(sample_sim)[dup[i,1]])
dup[,2] <- sapply(1:nrow(dup), function(i) dup[i,2] <- rownames(sample_sim)[dup[i,2]])
#now we can store duplicates along orignila, similarity and ID in data frame
duplicates.df <- data.frame(Date = rep(dates.d, nrow(dup)),
ID.original=dup[,2],
Original=texts[match(dup[,2],IDs)],
ID.duplicate=dup[,1],
Duplicate=texts[match(dup[,1],IDs)],
Similarity=sample_sim[as.matrix(dup)],stringsAsFactors=FALSE)
# additionally to the similarity the relative distance (original text from duplicate divided by character
#length of longer text) can be added
if(Rel.diff.on){
duplicates.df$Rel.diff. <- sapply(1:nrow(dup), function(i) adist(texts[match(dup[i,2],IDs)],texts[match(dup[i,1],IDs)], ignore.case = TRUE)/# string distance
max(c(nchar(texts[match(dup[i,2],IDs)]), nchar(texts[match(dup[i,1],IDs)]))))
}
cat("\rProcessing date ", as.character(dates.d)," ... ", nrow(dup), " duplicates found \t\t", sep="")
duplicates.df
}
else
{cat("\rProcessing date ", as.character(dates.d)," ... 0 duplicates found \t\t", sep="")}
})
#end loop
duplicates.df <- as.data.frame(rbindlist(duplicates.df))
if(Rel.diff.on){colnames(duplicates.df)[7] <- "Rel.diff."}
end.time <- Sys.time()
time.elapsed <- end.time - start.time
cat("\n Threshold = ", threshold, length(dates.d), "days processed...\n",nrow(duplicates.df[unique(duplicates.df$ID.duplicate),]), "duplicates found...\n", "In", format(time.elapsed, digits = 2, nsmall = 2))
return(duplicates.df)
}
meta_articles.df <- subset(meta_articles.df, Date < "2001-01-01"))
meta_articles.df <- subset(meta_articles.df, Date < "2001-01-01")
duplicates.df <- similarity_LN(texts = meta_articles.df$Article,
dates = meta_articles.df$Date,
IDs = meta_articles.df$ID,
threshold = 0.98,
Rel.diff.on = TRUE)
saveRDS(duplicates.df, file = "F:/Dropbox/SagarzazuLangerPersonalization_Data_201710/Interim/cleaning/duplicates.df_part1.RDS")
meta_articles.df <- merge(meta.df, articles.df, by = "ID")
meta_articles.df <- subset(meta_articles.df, Date >= "2001-01-01")
min(meta_articles.df$Date)
meta_articles.df <- merge(meta.df, articles.df, by = "ID")
meta_articles.df <- subset(meta_articles.df, Date >= "2001-01-01")
#'==================================================================================================================#
###' remove highly similar articles
#' Define function
similarity_LN <- function(texts, dates, IDs = NULL, threshold = 0.99, Rel.diff.on=FALSE) {
start.time <- Sys.time()
#first, we need to unique days so we can loop through them
dates.d <-unique(dates)
dates.d <-dates.d[order(dates.d)]
duplicates.df <- lapply(dates.d ,FUN=function(dates.d){
# convert to quanteda dfm
text.dfm <- dfm(texts[grep(dates.d, dates)], verbose = F, tolower = TRUE)
if(is.null(IDs)){IDs <- 1:length(texts)}
text.dfm@Dimnames$docs <- IDs[grep(dates.d, dates)]
#docnames(text.dfm)
sample_sim <- as.matrix(textstat_simil(text.dfm, selection=NULL , method = "correlation",
margin="documents"))
# which values are bigger than test_sim but smaller 1 (direct duplicates are already gone)
dup <- as.data.frame(which(sample_sim > threshold & sample_sim < 1, arr.ind=TRUE))
# every pair of duplicates is in here twice. This doesn't make sense of course so let's get rid of them
if (nrow(dup)>0){
dup[,3] <- dup[,1]+dup[,2]
dup <- dup[(which(duplicated(dup[,3])==FALSE)),]
dup <- dup[,-3]
# now the row/colnumbers are replaced by article IDs (which were stored as docnames)
dup[,1] <- sapply(1:nrow(dup), function(i) dup[i,1] <- rownames(sample_sim)[dup[i,1]])
dup[,2] <- sapply(1:nrow(dup), function(i) dup[i,2] <- rownames(sample_sim)[dup[i,2]])
#now we can store duplicates along orignila, similarity and ID in data frame
duplicates.df <- data.frame(Date = rep(dates.d, nrow(dup)),
ID.original=dup[,2],
Original=texts[match(dup[,2],IDs)],
ID.duplicate=dup[,1],
Duplicate=texts[match(dup[,1],IDs)],
Similarity=sample_sim[as.matrix(dup)],stringsAsFactors=FALSE)
# additionally to the similarity the relative distance (original text from duplicate divided by character
#length of longer text) can be added
if(Rel.diff.on){
duplicates.df$Rel.diff. <- sapply(1:nrow(dup), function(i) adist(texts[match(dup[i,2],IDs)],texts[match(dup[i,1],IDs)], ignore.case = TRUE)/# string distance
max(c(nchar(texts[match(dup[i,2],IDs)]), nchar(texts[match(dup[i,1],IDs)]))))
}
cat("\rProcessing date ", as.character(dates.d)," ... ", nrow(dup), " duplicates found [",
format((Sys.time()-start.time), digits = 2, nsmall = 2),"]\t\t", sep="")
duplicates.df
}
else
{cat("\rProcessing date ", as.character(dates.d)," ... 0 duplicates found  [",
format((Sys.time()-start.time), digits = 2, nsmall = 2),"]\t\t", sep="")}
})
#end loop
duplicates.df <- as.data.frame(rbindlist(duplicates.df))
if(Rel.diff.on){colnames(duplicates.df)[7] <- "Rel.diff."}
end.time <- Sys.time()
time.elapsed <- end.time - start.time
cat("\n Threshold = ", threshold, length(dates.d), "days processed...\n",nrow(duplicates.df[unique(duplicates.df$ID.duplicate),]), "duplicates found...\n", "In", format(time.elapsed, digits = 2, nsmall = 2))
return(duplicates.df)
}
duplicates.df <- similarity_LN(texts = meta_articles.df$Article,
dates = meta_articles.df$Date,
IDs = meta_articles.df$ID,
threshold = 0.98,
Rel.diff.on = TRUE)
saveRDS(duplicates.df, file = "F:/Dropbox/SagarzazuLangerPersonalization_Data_201710/Interim/cleaning/duplicates.df_part2.RDS")
saveRDS(duplicates.df, file = "F:/Dropbox/SagarzazuLangerPersonalization_Data_201710/Interim/cleaning/duplicates.df_part2.RDS")
duplicates.df1 <- readRDS("F:/Dropbox/SagarzazuLangerPersonalization_Data_201710/Interim/cleaning/duplicates.df_part1.RDS")
duplicates.df2 <- readRDS("F:/Dropbox/SagarzazuLangerPersonalization_Data_201710/Interim/cleaning/duplicates.df_part2.RDS")
duplicates.df <- rbind(duplicates.df1, duplicates.df2)
rm(duplicates.df1,duplicates.df2)
duplicates.df <- duplicates.df[duplicates.df$Rel.diff. < 0.2, ]
duplicates.df$ID.original <- as.integer(duplicates.df$ID.original)
duplicates.df$ID.duplicate <- as.integer(duplicates.df$ID.duplicate)
table(is.na(meta_articles.df$`Source.file(s)`))
for(i in 1:nrow(duplicates.df)){
meta_articles.df$`Source.file(s)`[meta_articles.df$ID == duplicates.df$ID.original[i]] <-        #replace source files
paste(meta_articles.df$`Source.file(s)`[meta_articles.df$ID == duplicates.df$ID.original[i]],  #with orig. source files
meta_articles.df$`Source.file(s)`[meta_articles.df$ID == duplicates.df$ID.duplicate[i]], #plus dup. source files
sep = ", ")
meta_articles.df$`Source.file(s)`[meta_articles.df$ID == duplicates.df$ID.duplicate[i]] <- NA    #flag for removal
cat("\r", scales::percent(i/nrow(duplicates.df)), "\t\t")
}
nrow(meta_articles.df)-nrow(meta_articles.df[!is.na(meta_articles.df$`Source.file(s)`),])
meta_articles.df <- meta_articles.df[!is.na(meta_articles.df$`Source.file(s)`),]
Dup <- meta_articles.df[, "Source.file(s)"]
Dup <- as.data.frame(splitstackshape::cSplit(Dup, "Source.file(s)", ","), stringsAsfactors=FALSE)
Dup <- as.data.frame(t(apply(Dup, 1, function(x) replace(x, duplicated(x), NA))),stringsAsfactors=FALSE)
Dup <- apply(Dup, 1, function(x) toString(na.omit(x)))
meta_articles.df$`Source.file(s)2` <- Dup
table(meta_articles.df$`Source.file(s)2` == meta_articles.df$`Source.file(s)`)
library(dplyr)
library(ggplot2)
library(quanteda)
library(data.table)
invisible(utils::memory.limit(70000))
meta.df <- readRDS("F:/Dropbox/SagarzazuLangerPersonalization_Data_201710/Dataset/meta.df_1992-2016_clean.RDS")
articles.df <- readRDS("F:/Dropbox/SagarzazuLangerPersonalization_Data_201710/Dataset/articles.df_1992-2016_clean.RDS")
meta_articles.df <- merge(meta.df, articles.df, by = "ID")
duplicates.df1 <- readRDS("F:/Dropbox/SagarzazuLangerPersonalization_Data_201710/Interim/cleaning/duplicates.df_part1.RDS")
duplicates.df2 <- readRDS("F:/Dropbox/SagarzazuLangerPersonalization_Data_201710/Interim/cleaning/duplicates.df_part2.RDS")
duplicates.df <- rbind(duplicates.df1, duplicates.df2);rm(duplicates.df1,duplicates.df2)
#' exclude relative Levenshtein distance larger than 20%
duplicates.df <- duplicates.df[duplicates.df$Rel.diff. < 0.2, ]
duplicates.df$ID.original <- as.integer(duplicates.df$ID.original)
duplicates.df$ID.duplicate <- as.integer(duplicates.df$ID.duplicate)
table(is.na(meta_articles.df$`Source.file(s)`))
for(i in 1:nrow(duplicates.df)){
meta_articles.df$`Source.file(s)`[meta_articles.df$ID == duplicates.df$ID.original[i]] <-        #replace source files
paste(meta_articles.df$`Source.file(s)`[meta_articles.df$ID == duplicates.df$ID.original[i]],  #with orig. source files
meta_articles.df$`Source.file(s)`[meta_articles.df$ID == duplicates.df$ID.duplicate[i]], #plus dup. source files
sep = ", ")
meta_articles.df$`Source.file(s)`[meta_articles.df$ID == duplicates.df$ID.duplicate[i]] <- NA    #flag for removal
cat("\r", scales::percent(i/nrow(duplicates.df)), "\t\t")
}
#' remove duplicates
nrow(meta_articles.df)-nrow(meta_articles.df[!is.na(meta_articles.df$`Source.file(s)`),]) # how many are removed
meta_articles.df <- meta_articles.df[!is.na(meta_articles.df$`Source.file(s)`),]
# Now we get rid of in-row duplicates in `Source.file(s)`
Dup <- meta_articles.df[, "Source.file(s)"]
Dup <- as.data.frame(splitstackshape::cSplit(Dup, "Source.file(s)", ","), stringsAsfactors=FALSE)
Dup <- as.data.frame(t(apply(Dup, 1, function(x) replace(x, duplicated(x), NA))),stringsAsfactors=FALSE)
Dup <- apply(Dup, 1, function(x) toString(na.omit(x)))
meta_articles.df$`Source.file(s)2` <- Dup
table(meta_articles.df$`Source.file(s)2` == meta_articles.df$`Source.file(s)`) #should not be higher than removed rows
report <- xlsx::read.xlsx("F:/Dropbox/SagarzazuLangerPersonalization_Data_201710/cleaning report.xlsx", sheetName = "cleaner")
temp <- count(meta_articles.df, dates=format(meta_articles.df$Date, format = "%Y"))
colnames(temp) <- c("Year", "rm similar articles")
report <- merge(report,temp,by="Year")
View(report)
xlsx::write.xlsx(report, "F:/Dropbox/SagarzazuLangerPersonalization_Data_201710/cleaning report.xlsx", sheetName = "cleaner",
col.names = TRUE, row.names = FALSE, append = FALSE)
colnames(meta_articles.df)
meta_articles.df$`Source.file(s)` <- Dup
table(meta_articles.df$`Source.file(s)2` == meta_articles.df$`Source.file(s)`) #should not be higher than removed rows
colnames(meta_articles.df)
#' meta
meta.df <- meta_articles.df[, c("ID", "Source.file(s)", "Nr. of hits", "Newspaper", "Date", "Length",
"Section", "Author", "Edition", "Headline")]
#' articles
articles.df <- meta_articles.df[, c("ID","Article")]
#' and save
saveRDS(meta.df,
file = "F:/Dropbox/SagarzazuLangerPersonalization_Data_201710/Dataset/meta.df_1992-2016_clean2.RDS")
saveRDS(articles.df,
file = "F:/Dropbox/SagarzazuLangerPersonalization_Data_201710/Dataset/articles.df_1992-2016_clean2.RDS")
paragraphs.df <- readRDS("F:/Dropbox/SagarzazuLangerPersonalization_Data_201710/Dataset/paragraphs.df.RDS")
paragraphs.df_clean <- paragraphs.df[paragraphs.df$Art_ID %in% meta.df$ID,]
if(all.equal(unique(paragraphs.df_clean$Art_ID), sort(meta.df$ID))){
saveRDS(paragraphs.df_clean,
file = "F:/Dropbox/SagarzazuLangerPersonalization_Data_201710/Dataset/paragraphs.m_1992-2016_clean2.RDS")
cat(TRUE)
}
paragraphs.df <- readRDS("F:/Dropbox/SagarzazuLangerPersonalization_Data_201710/Dataset/paragraphs.m_1992-2016_clean.RDS")
meta.m <- readRDS("F:/Dropbox/SagarzazuLangerPersonalization_Data_201710/Dataset/meta.df_1992-2016_clean.RDS")
nrow(paragraphs.df[!duplicated(paragraphs.df$Art_ID),])
library(quanteda)
library(spacyr)
library(dplyr)
spacy_initialize(model = "en_core_web_lg", python_executable = "C:/Users/binis/Anaconda3/python.exe")
Sys.time()
Sys.time()
library(quanteda)
library(spacyr)
library(dplyr)
spacy_initialize(model = "en_core_web_lg", python_executable = "C:/Users/binis/Anaconda3/python.exe")
paragraphs.df <- readRDS("F:/Dropbox/paperKwic/Topicmodels/leader_paragraphs_clean.RDS")
paragraphs.df <- readRDS("F:/Dropbox/SagarzazuLangerPersonalization_Data_201710/Dataset/Leader/leader_paragraphs_clean.RDS")
paragraphs.df_MAY <- paragraphs.df[!is.na(paragraphs.df$leader.id.20),]
#' POS tag in spacyr (this is not perfect but the only way I could think of to pass the Par_IDs on, other wise they are called text1 etc.)
paragraphs.df_MAY$Par_ID <- as.character(paragraphs.df_MAY$Par_ID)
paragraphs.c_MAY <- corpus(paragraphs.df_MAY,
docid_field = "Par_ID",
text_field = "Paragraph",
metacorpus = paragraphs.df_MAY)
paragraphs.POS_MAY <- spacy_parse(paragraphs.c_MAY,
pos = TRUE,
tag = TRUE,
lemma = TRUE,
entity = TRUE,
dependency = TRUE)
#' save interim result
saveRDS(paragraphs.POS_MAY, "F:/Dropbox/paperKwic/Topicmodels/MAY_paragraphs_POS.RDS")
Sys.time()
library(quanteda)
library(spacyr)
library(dplyr)
#spacy_initialize(model = "en_core_web_lg", python_executable = "C:/Users/binis/Anaconda3/python.exe")
paragraphs.df <- readRDS("F:/Dropbox/SagarzazuLangerPersonalization_Data_201710/Dataset/Leader/leader_paragraphs_clean.RDS")
paragraphs.df_MAY <- paragraphs.df[!is.na(paragraphs.df$leader.id.20),]
#' POS tag in spacyr (this is not perfect but the only way I could think of to pass the Par_IDs on, other wise they are called text1 etc.)
paragraphs.df_MAY$Par_ID <- as.character(paragraphs.df_MAY$Par_ID)
paragraphs.c_MAY <- corpus(paragraphs.df_MAY,
docid_field = "Par_ID",
text_field = "Paragraph",
metacorpus = paragraphs.df_MAY)
#paragraphs.POS_MAY <- spacy_parse(paragraphs.c_MAY,
#                                        pos = TRUE,
#                                       tag = TRUE,
#                                      lemma = TRUE,
#                                     entity = TRUE,
#                                    dependency = TRUE)
#' save interim result
#saveRDS(paragraphs.POS_MAY, "F:/Dropbox/paperKwic/Topicmodels/MAY_paragraphs_POS.RDS")
paragraphs.POS_MAY <- readRDS("F:/Dropbox/paperKwic/Topicmodels/MAY_paragraphs_POS.RDS")
#' relabel token_id since it starts from 1 in every new document
paragraphs.POS_MAY$token_id <- 1:nrow(paragraphs.POS_MAY)
#' how is 'May' categorised?
paragraphs.POS_MAY[grep("MAY|May", paragraphs.POS_MAY$token),] %>%
group_by(entity) %>%
summarise (n = n()) %>%
mutate(pct = scales::percent(n / sum(n)))
#' create condition 1 (May is either person, sibject or possesive)
paragraphs.POS_MAY$condition1 <- FALSE
paragraphs.POS_MAY$condition1[grepl("MAY|May", paragraphs.POS_MAY$token) & (  # May is word
grepl("PERSON", paragraphs.POS_MAY$entity) | # Either entity is Person
grepl("poss|nsubj", paragraphs.POS_MAY$dep_rel))] <- TRUE # Or dependency is subject or possesive (May's)
table(paragraphs.POS_MAY$condition1)
#' is the word before MAY = Mrs or Theresa
x <- paragraphs.POS_MAY[grep("MAY|May", paragraphs.POS_MAY$token),] # word=May
x <- paragraphs.POS_MAY[paragraphs.POS_MAY$token_id %in% (x$token_id-1),] # word=May-1
#' most common words before MAY
sort(table(x$token), decreasing = TRUE)[1:25]
# create label
x$label <- FALSE
x$label[grep("^Mrs$|Theresa", x$token, ignore.case = TRUE)] <- TRUE
table(x$label)
#' drop uses of May different from Mrs and Theresa
x <- x[x$label == TRUE, ]
paragraphs.POS_MAY$condition1[paragraphs.POS_MAY$token_id %in% x$token_id] <- TRUE # May is word, word before is Mrs
rm(x)
table(paragraphs.POS_MAY$condition1)
#' has MAY been used as person
person <- paragraphs.POS_MAY %>%
group_by(doc_id) %>%
summarise(p = mean(condition1))
nrow(person[person$p > 0,]) # May is person i.e. 1. entity = Date, 2. dependency ≠ subject, or 3. word before 'May' ≠ 'Theresa' or 'Mrs'
#' create second condition May ≠ Date
paragraphs.POS_MAY$condition2 <- TRUE
paragraphs.POS_MAY$condition2[grepl("MAY|May", paragraphs.POS_MAY$token) &
grepl("DATE", paragraphs.POS_MAY$entity)] <- FALSE
table(paragraphs.POS_MAY$condition2)
#' has MAY been used as date
date <- paragraphs.POS_MAY %>%
group_by(doc_id) %>%
summarise(d = mean(condition2))
nrow(date[date$d < 1,]) # May is Date
#' in which paragraphs was May used as date but not as Person?
test_MAY <- merge(person, date, by = "doc_id")
test_MAY$test <- TRUE
test_MAY$test[test_MAY$d < 1 & test_MAY$p == 0] <- FALSE # in which paragraphs was May sometimes date & never person
table(test_MAY$test)
#' create condition 3: Within Date but never person is word before May is not Mr or in,
#' word after not a number (i.e. year/month)
x <- paragraphs.POS_MAY[(paragraphs.POS_MAY$doc_id %in% test_MAY$doc_id[test_MAY$test == FALSE]),]
x <- x[grepl("MAY|May", x$token),]
x <- paragraphs.POS_MAY[paragraphs.POS_MAY$token_id %in% (x$token_id-1),]
x$condition3 <- TRUE
x$condition3[grep("^in$|^Mr$", x$token, ignore.case = TRUE)] <- FALSE
table(x$condition3)
x2 <- paragraphs.POS_MAY[paragraphs.POS_MAY$token_id %in% (x$token_id+2),] # two after x so one after May
x2$condition3 <- TRUE
x2$condition3[grep("[[:digit:]]", x2$token, ignore.case = TRUE)] <- FALSE
table(x2$condition3)
x <- rbind(x,x2);rm(x2)
x <- x[x$condition3 == FALSE, ]
paragraphs.POS_MAY$condition3 <- TRUE
paragraphs.POS_MAY$condition3[paragraphs.POS_MAY$doc_id %in% x$doc_id] <- FALSE
table(paragraphs.POS_MAY$condition3)
#' Which paragraphs are relabeled, which would be kept?
paragraphs.df_relabels <- paragraphs.df_MAY[paragraphs.df_MAY$Par_ID %in%
paragraphs.POS_MAY$doc_id[paragraphs.POS_MAY$condition3 == FALSE],]
paragraphs.df_noRelabels <- paragraphs.df_MAY[!paragraphs.df_MAY$Par_ID %in%
paragraphs.POS_MAY$doc_id[paragraphs.POS_MAY$condition3 == FALSE],]
nrow(paragraphs.df_relabels)+nrow(paragraphs.df_noRelabels)==nrow(paragraphs.df_MAY)
#' how much of he May sample is removed in percent?
scales::percent(nrow(paragraphs.df_relabels)/nrow(paragraphs.df_MAY))
paragraphs.df$leader.id.20[paragraphs.df$Par_ID %in% as.integer(paragraphs.df_relabels$Par_ID)] <- NA
paragraphs.df$leader.id.20[paragraphs.df$Par_ID %in% c(17849182, 20004214, 20037844, 20042732,
20069302, 20127102, 20163095, 20054956,
17843338, 20022138, 15146711, 19967455,
20110705, 18716007, 20036430, 20093460,
18716272)] <- NA
paragraphs.df_Miliband <- paragraphs.df[!is.na(paragraphs.df$leader.id.3),]
paragraphs.df_Miliband$label <- FALSE
condition <- paragraphs.df_Miliband[grepl("David Miliband", paragraphs.df_Miliband$Paragraph, ignore.case = TRUE) &
!grepl("Edward Miliband|Ed Miliband", paragraphs.df_Miliband$Paragraph, ignore.case = TRUE),]
table(is.na(condition$leader.id.3))
paragraphs.df$leader.id.3[paragraphs.df$Par_ID %in% condition$Par_ID] <- NA
leaders <- read.csv("F:/Dropbox/SagarzazuLangerPersonalization_Data/Variables/party_leaders.csv", stringsAsFactors = FALSE)
leaders <- leaders[!leaders$Spouse == "",]
for(i in 1:nrow(leaders)){
paragraphs.df_temp <- paragraphs.df[!is.na(paragraphs.df[,grep(paste0("leader.id.", leaders$leader.id[i], "$"),
colnames(paragraphs.df))]),]
condition <- paragraphs.df_temp[grepl(leaders$Spouse[i], paragraphs.df_temp$Paragraph, ignore.case = TRUE) &
!grepl(paste(leaders$First.name[i], leaders$Last.name[i]),
paragraphs.df_temp$Paragraph, ignore.case = TRUE), ]
cat(leaders$Last.name[i], "\n")
print(table(is.na(condition[, grep(paste0("leader.id.", leaders$leader.id[i], "$"),
colnames(paragraphs.df))])))
paragraphs.df[paragraphs.df$Par_ID %in% condition$Par_ID,
grep(paste0("leader.id.", leaders$leader.id[i], "$"),
colnames(paragraphs.df))] <- NA
}
paragraphs.df_leader <- paragraphs.df[rowSums(is.na(paragraphs.df[,grep("leader.id.", colnames(paragraphs.df))])) #count NAs in additional columns
<ncol(paragraphs.df[,grep("leader.id.", colnames(paragraphs.df))]),] # number of NAs should be smaller than number of additional columns
nrow(paragraphs.df)-nrow(paragraphs.df_leader)
filename <- "F:/Dropbox/paperKwic/Code/Cleaning_tables.xlsx"
cleaning <- xlsx::read.xlsx(filename, sheetName = "Paragraphs")
cleaning_art <- xlsx::read.xlsx(filename, sheetName = "Articles")
temp <- count(paragraphs.df_leader, dates=format(paragraphs.df_leader$Date, format = "%Y"))
colnames(temp) <- c("dates","special relabels")
temp_art <- count(paragraphs.df_leader[!duplicated(paragraphs.df_leader$Art_ID),],
dates=format(paragraphs.df_leader$Date[!duplicated(paragraphs.df_leader$Art_ID)], format = "%Y"))
colnames(temp_art) <- c("dates","special relabels")
cleaning <- merge(cleaning, temp, by = "dates", all=TRUE)
cleaning_art <- merge(cleaning_art, temp_art, by = "dates", all=TRUE)
cleaning$total.change.pct <- (cleaning$untouched-cleaning$`special relabels`)/(cleaning$untouched+cleaning$`special relabels`)
cleaning$fine.change.pct <- (cleaning$cleaned.no.mentions-cleaning$`special relabels`)/(cleaning$cleaned.no.mentions+cleaning$`special relabels`)
cleaning_art$total.change.pct <- (cleaning_art$untouched-cleaning_art$`special relabels`)/(cleaning_art$untouched+cleaning_art$`special relabels`)
cleaning_art$fine.change.pct <- (cleaning_art$cleaned.no.mentions-cleaning_art$`special relabels`)/(cleaning_art$cleaned.no.mentions+cleaning_art$`special relabels`)
cleaning <- cleaning[, c(1:5, 8, 6,7)]
colnames(cleaning)[1:5]
colnames(cleaning)
View(cleaning)
View(cleaning_art)
Dates_para <- count(paragraphs.df_leader, dates=format(paragraphs.df_leader$Date, format = "%Y"))
Dates_para <- Dates_para[!is.na(Dates_para$dates),]
paragraphs.df_leader_unique.art <- paragraphs.df_leader[!duplicated(paragraphs.df_leader$Art_ID),]
Dates_arti <- count(paragraphs.df_leader_unique.art, dates=format(paragraphs.df_leader_unique.art$Date, format = "%Y"))
Dates_arti <- Dates_arti[!is.na(Dates_arti$dates),]
Dates <- merge(Dates_arti, Dates_para, by="dates")
colnames(Dates) <- c("dates", "articles", "paragraphs" )
Dates <- Dates[!is.na(Dates$dates),]
min(na.omit(paragraphs.df_leader$Date))
max(na.omit(paragraphs.df_leader$Date))
#write.xlsx(Dates, "Articles&Paragraphs per year.xlsx", sheetName = "Paragraphs",
#           col.names = TRUE, row.names = FALSE, append = FALSE)
library("ggplot2")
#windows(width=16, height=10) #open window
ggplot() +
geom_line(data=Dates_para, # first line
aes(x=dates, y=n, group = 1, colour="Paragraphs with leader per year"), size = 1.5)+
geom_line(data=Dates_arti, #second line
aes(x=dates, y=n, group = 1, colour="Articles with leader per year"), size = 1.5)+ #scale_y_continuous(limits = c(0, 70000))+
xlab("Years")+ #axis labels
#scale_colour_grey(name="Error Bars", start = 0, end = .9) +
scale_colour_manual("",
breaks = c("Paragraphs with leader per year", "Articles with leader per year"),
values=c("black","grey"))+
theme_bw()+#make black and white
theme(axis.text.x = element_text(angle = 90, hjust = 1),
axis.title.y = element_blank(),
legend.position = c(0.2,0.9), legend.background = element_rect(fill=alpha('white', 0.2))) #place legend
windows(width=16, height=10) #open window
ggplot() +
geom_line(data=Dates_para, # first line
aes(x=dates, y=n, group = 1, colour="Paragraphs with leader per year"), size = 1.5)+
geom_line(data=Dates_arti, #second line
aes(x=dates, y=n, group = 1, colour="Articles with leader per year"), size = 1.5)+ #scale_y_continuous(limits = c(0, 70000))+
xlab("Years")+ #axis labels
#scale_colour_grey(name="Error Bars", start = 0, end = .9) +
scale_colour_manual("",
breaks = c("Paragraphs with leader per year", "Articles with leader per year"),
values=c("black","grey"))+
theme_bw()+#make black and white
theme(axis.text.x = element_text(angle = 90, hjust = 1),
axis.title.y = element_blank(),
legend.position = c(0.2,0.9), legend.background = element_rect(fill=alpha('white', 0.2))) #place legend
nrow(paragraphs.df)/nrow(paragraphs.df_unique.art)
nrow(paragraphs.df)/nrow(paragraphs.df_leader_unique.art)
paragraphs.df_full <- readRDS("F:/Dropbox/SagarzazuLangerPersonalization_Data_201710/Dataset/paragraphs.m_1992-2016_clean.RDS")
paragraphs.df_full <- paragraphs.df_full[paragraphs.df_full$Art_ID %in% paragraphs.df$Art_ID,]
nrow(paragraphs.df_full)/nrow(paragraphs.df_full[unique(paragraphs.df_full$Art_ID)])
rowSums(is.na(paragraphs.df[1,grep("leader.id.", colnames(paragraphs.df))]))
is.na(paragraphs.df[1,grep("leader.id.", colnames(paragraphs.df))])
rowSums(!is.na(paragraphs.df[1,grep("leader.id.", colnames(paragraphs.df))]))
rowSums(!is.na(paragraphs.df[1,grep("leader.id.", colnames(paragraphs.df))]))
paragraphs.df$mentions <- rowSums(!is.na(paragraphs.df[,grep("leader.id.", colnames(paragraphs.df))]))
View(paragraphs.df)
paragraphs.df_leader$mentions <- rowSums(!is.na(paragraphs.df_leader[,grep("leader.id.", colnames(paragraphs.df_leader))]))
View(paragraphs.df_leader)
mean(paragraphs.df_leader$mentions)
devtools::install_github("statsmaths/cleanNLP")
library(cleanNLP)
download_core_nlp()
init_coreNLP(anno_level = 2L, lib_location = lib_loc)
obj <- run_annotators(text, as_strings = TRUE, backend = "coreNLP")
init_coreNLP(anno_level = 2L, lib_location = lib_loc)
init_coreNLP(anno_level = 2L, lib_location = "C:/Users/binis/Documents/R/win-library/3.4/cleanNLP/extdata")
init_coreNLP(anno_level = 2L, lib_location = "C:/Users/binis/Documents/R/win-library/3.4/cleanNLP/extdata")
library(cleanNLP)
init_coreNLP(anno_level = 2L, lib_location = "C:/Users/binis/Documents/R/win-library/3.4/cleanNLP/extdata")
install.packages("rJava", type = "source")
install.packages("rJava", type = "source")
library(cleanNLP)
init_coreNLP(anno_level = 2L, lib_location = "C:/Users/binis/Documents/R/win-library/3.4/cleanNLP/extdata")
.jinit()
library(rJava)
.jinit()
.jcall("java/lang/System", "S", "getProperty", "java.runtime.version")
options(java.home="C:/Program Files/Java/jre1.8.0_151")
.jinit()
.jcall("java/lang/System", "S", "getProperty", "java.runtime.version")
options(java.home="C:/Program Files/Java/jre1.8.0_151")
library(cleanNLP)
library(rJava)
.jinit()
.jcall("java/lang/System", "S", "getProperty", "java.runtime.version")
init_coreNLP(anno_level = 2L, lib_location = "C:/Users/binis/Documents/R/win-library/3.4/cleanNLP/extdata")
download_core_nlp()
init_coreNLP(anno_level = 2L, lib_location = lib_loc)
init_coreNLP(language = "en", anno_level = 2L,
lib_location = NULL)
init_coreNLP(language = "en", anno_level = 2L, mem = "20g"
lib_location = NULL)
init_coreNLP(language = "en", anno_level = 2L, mem = "20g",
lib_location = NULL)
init_coreNLP(language = "en", anno_level = 2L, mem = "2g",
lib_location = NULL)
options(java.home="C:/Program Files/Java/jre1.8.0_151") #for some reason R picks up old version of Java
library(cleanNLP)
library(rJava)
.jinit()
.jcall("java/lang/System", "S", "getProperty", "java.runtime.version")
options(java.parameters = "-Xmx20g" )
init_coreNLP(language = "en", anno_level = 2L, mem = "10g",
lib_location = NULL)
library(readtext)
PPstudies <- readtext("F:/Dropbox/PhD/Literature/Protest/Paradigm studies/*.pdf")
View(PPstudies)
PPstudies_inter <- PPstudies[grep("demonst", PPstudies$text), ]
PPstudies_inter <- PPstudies[grepl("demonst", PPstudies$text)&
grepl("protest", PPstudies$text)&
grepl("march", PPstudies$text), ]
View(PPstudies_inter)
library(quanteda)
PPstudies_inter.kwic <- kwic(PPstudies_inter, pattern = "demonst*|protest|march*", window = 5, valuetype = "glob",
case_insensitive = TRUE)
PPstudies_inter.kwic <- kwic(PPstudies_inter$text, pattern = "demonst*|protest|march*",
window = 5, valuetype = "glob", case_insensitive = TRUE)
View(PPstudies_inter.kwic)
PPstudies_inter <- PPstudies[grepl("demonstration", PPstudies$text)&
grepl("protest", PPstudies$text)&
grepl("march", PPstudies$text), ]
PPstudies_inter.kwic <- kwic(PPstudies_inter$text, pattern = "demonstration*|protest|march*",
window = 5, valuetype = "glob", case_insensitive = TRUE)
View(PPstudies_inter.kwic)
View(PPstudies_inter)
library("LexisNexisTools", lib.loc="~/R/win-library/3.4")
x <- rename_LNfiles("F:/Dropbox/Alex oOo Johannes/Database 2001 - Copy/", recursive = TRUE, report = TRUE)
?rename_LNfiles
library("devtools")
library("roxygen2")
# create the package in wd
setwd("C:/Users/binis/Documents/GitHub/LexisNexisTools")
roxygenise(clean = TRUE)
